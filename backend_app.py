from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import asyncio
from fastapi.middleware.cors import CORSMiddleware

# Importar el agente y sus componentes desde el archivo modificado
# Aseg√∫rate de que 'agent_gemini_current.py' est√© en el mismo directorio
from agent import (
    agent_executor,
    add_to_history,
    get_context,
    test_connection,
)

# Definir el modelo de datos para la entrada de la API
class ChatRequest(BaseModel):
    query: str

# Inicializar la aplicaci√≥n FastAPI
app = FastAPI(
    title="Agente Conversacional LangChain (Gemini Backend)",
    description="Backend para interactuar con el agente LangChain multi-fuente, potenciado por Google Gemini.",
    version="1.0.0",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Permite todas las or√≠genes. En producci√≥n, deber√≠as restringirlo a tu dominio de frontend.
    allow_credentials=True,
    allow_methods=["*"],  # Permite todos los m√©todos (GET, POST, etc.)
    allow_headers=["*"],  # Permite todos los encabezados
)

# Evento de inicio: probar la conexi√≥n con Gemini
@app.on_event("startup")
async def startup_event():
    print("üöÄ Iniciando backend...")
    if not test_connection():
        print("‚ùå La conexi√≥n con Gemini fall√≥ al iniciar. El backend podr√≠a no funcionar correctamente.")
        # Opcional: podr√≠as detener la aplicaci√≥n si la conexi√≥n es cr√≠tica
        # raise RuntimeError("No se pudo conectar con Gemini. Verifica tu API key y cuota.")
    else:
        print("‚úÖ Backend iniciado y conectado a Gemini.")

# Ruta principal para verificar que el backend est√° funcionando
@app.get("/")
async def read_root():
    return {"message": "Bienvenido al Agente Conversacional LangChain. Usa /chat para interactuar."}

# Ruta para interactuar con el agente
@app.post("/chat")
async def chat_with_agent(request: ChatRequest):
    user_query = request.query
    
    try:
        # Obtener el contexto de conversaci√≥n actual
        context = get_context()
        
        # Invocar al agente con la pregunta del usuario y el contexto
        # LangChain AgentExecutor.invoke es s√≠ncrono, as√≠ que lo ejecutamos en un executor
        # para no bloquear el bucle de eventos de FastAPI.
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None, # Usa el ThreadPoolExecutor por defecto
            agent_executor.invoke,
            {"input": user_query, "conversation_context": context}
        )
        
        agent_response = response["output"]
        
        # A√±adir el intercambio al historial de conversaci√≥n
        add_to_history(user_query, agent_response)
        
        return {"response": agent_response}
    except Exception as e:
        print(f"‚ùå Error al procesar la petici√≥n: {e}")
        raise HTTPException(status_code=500, detail=f"Error interno del servidor: {str(e)}")

# Para ejecutar la aplicaci√≥n directamente (opcional, uvicorn se usa m√°s com√∫nmente)
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
